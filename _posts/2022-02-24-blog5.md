---
layout: post
title: Blog Post 5 Image Classification
---

In this blog post, we would show you how to train a machine learning image classification algorithm. We will be classifying cats and dogs using the dataset [linked here](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip). We will be using the `tensorflow` package to train neural network models and exploring ways including *data augmentation* and *transfer learning* to improve our models. 

This blog post is based on the TensorFlow [Transfer Learning tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning).

## Load Packages and Obtain Data

First, we start by loading required packages, obtaining dataset from the url and creating *train*, *validation* and *test* tensorflow datasets. 


```python
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import utils, datasets, layers, models
```


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.



```python
#reformat the data to help with rapidly reading the data
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

#### Visualize random images of cats and dogs


```python
def visualize_data():
  #create a 2 by 3 grid to display cat and dog images
  fig, axes = plt.subplots(2,3, figsize = (8,6))
  images, labels = list(train_dataset.take(1))[0]

  #plot images of cats
  for i,idx in enumerate(np.where(labels==0)[0][:3]):
    axes[0,i].imshow(images[idx]/255.0)
    axes[0,i].set_title('cat')
    axes[0,i].axis('off')

  #plot images of dogs
  for i,idx in enumerate(np.where(labels==1)[0][:3]):
    axes[1,i].imshow(images[idx]/255.0)
    axes[1,i].set_title('dog')
    axes[1,i].axis('off')
  fig.show()
```


```python
visualize_data()
```


    
![png](../images/image-classification/PIC16B_Blog_Post_4_9_0.png)
    


#### Check Label frequencies

When working with binary classification models, we can easily obtain a "high" accuracy by predicting all of the data to be the majority class. Thus, we need to check the frequency of the "dog" and "cat" labels and understand our baseline accuracy score.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```


```python
cat_counts = 0
dog_counts = 0
for label in labels_iterator:
  if label == 0:
    cat_counts += 1
  else:
    dog_counts += 1
print(f"The train dataset contains {cat_counts} samples of cats, which is {cat_counts/(cat_counts+dog_counts)*100}% of the whole dataset")
print(f"It contains {dog_counts} samples of dogs, which is {dog_counts/(cat_counts+dog_counts)*100}% of the whole dataset")
```

    The train dataset contains 1000 samples of cats, which is 50.0% of the whole dataset
    It contains 1000 samples of dogs, which is 50.0% of the whole dataset


We see that both the cat and dog labels have 1000 samples, and thus our baseline accuracy is 50%. When we are evaluating our models, we would be aware of this baseline score for comparison.

## First Model

We start by building a simple neurla network models with different tyeps of layers, including `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, and `Dropout`.


```python
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.01),
    layers.Dense(2) # number of classes
])
```


```python
#summary of the model 
model1.summary()
```

    Model: "sequential_16"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_40 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_28 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_41 (Conv2D)          (None, 77, 77, 64)        18496     
                                                                     
     max_pooling2d_29 (MaxPoolin  (None, 38, 38, 64)       0         
     g2D)                                                            
                                                                     
     conv2d_42 (Conv2D)          (None, 36, 36, 128)       73856     
                                                                     
     flatten_12 (Flatten)        (None, 165888)            0         
                                                                     
     dense_28 (Dense)            (None, 128)               21233792  
                                                                     
     dropout_3 (Dropout)         (None, 128)               0         
                                                                     
     dense_29 (Dense)            (None, 2)                 258       
                                                                     
    =================================================================
    Total params: 21,327,298
    Trainable params: 21,327,298
    Non-trainable params: 0
    _________________________________________________________________



```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

```


```python
history1 = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 8s 105ms/step - loss: 76.4337 - accuracy: 0.5320 - val_loss: 0.6725 - val_accuracy: 0.5928
    Epoch 2/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.6091 - accuracy: 0.6830 - val_loss: 0.6671 - val_accuracy: 0.6200
    Epoch 3/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.5401 - accuracy: 0.7465 - val_loss: 0.7722 - val_accuracy: 0.5767
    Epoch 4/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.3993 - accuracy: 0.8155 - val_loss: 1.3734 - val_accuracy: 0.6275
    Epoch 5/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.2500 - accuracy: 0.8985 - val_loss: 1.1738 - val_accuracy: 0.6349
    Epoch 6/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.3348 - accuracy: 0.8945 - val_loss: 1.3864 - val_accuracy: 0.5891
    Epoch 7/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1800 - accuracy: 0.9330 - val_loss: 1.6055 - val_accuracy: 0.5953
    Epoch 8/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0788 - accuracy: 0.9715 - val_loss: 1.8142 - val_accuracy: 0.6052
    Epoch 9/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2381 - accuracy: 0.9445 - val_loss: 1.5978 - val_accuracy: 0.6139
    Epoch 10/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0737 - accuracy: 0.9820 - val_loss: 1.5120 - val_accuracy: 0.6064
    Epoch 11/20
    63/63 [==============================] - 8s 118ms/step - loss: 0.0368 - accuracy: 0.9905 - val_loss: 1.8685 - val_accuracy: 0.6002
    Epoch 12/20
    63/63 [==============================] - 7s 95ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 2.3628 - val_accuracy: 0.6114
    Epoch 13/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0196 - accuracy: 0.9965 - val_loss: 2.1546 - val_accuracy: 0.5656
    Epoch 14/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0082 - accuracy: 0.9990 - val_loss: 2.4034 - val_accuracy: 0.5780
    Epoch 15/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.6234 - val_accuracy: 0.5743
    Epoch 16/20
    63/63 [==============================] - 6s 94ms/step - loss: 6.3087e-04 - accuracy: 1.0000 - val_loss: 2.7018 - val_accuracy: 0.5755
    Epoch 17/20
    63/63 [==============================] - 6s 94ms/step - loss: 3.5639e-04 - accuracy: 1.0000 - val_loss: 2.6429 - val_accuracy: 0.5916
    Epoch 18/20
    63/63 [==============================] - 6s 93ms/step - loss: 2.1885e-04 - accuracy: 1.0000 - val_loss: 2.7116 - val_accuracy: 0.5941
    Epoch 19/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 2.7196 - val_accuracy: 0.5743
    Epoch 20/20
    63/63 [==============================] - 6s 92ms/step - loss: 8.4242e-04 - accuracy: 1.0000 - val_loss: 2.9512 - val_accuracy: 0.5705



```python
plt.plot(history1.history["accuracy"], label = "training")
plt.plot(history1.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fb447e823d0>




    
![png](../images/image-classification/PIC16B_Blog_Post_4_21_1.png)
    


The validation accuracy of my model stabilized **between 56% and 61%** during training, which is somewhat higher than our baseline of 50%. However, the trainining accuracy continue increases to 1, which is a sign of overfitting. 

## Model With Data Augmentation

In this section, we explore models with data augmentation. Data augmentation is the pratice of including modified copies of the same image in the training dataset. We can include such types of transformed images to help model learn *invariant* features of our input images. Here we will make use of the two functions `tf.keras.layers.RandomFlip` and `tf.keras.layers.RandomRotation`.

### Random Flip

We would start by show a few images of result of random flip. In the plot below, the first image indicates of original image and the remaining ones show differnet flipped versions of the same image.


```python
images, labels = list(train_dataset.take(1))[0]
first_image = images[0]
fig, axes = plt.subplots(3,3, figsize=(10,10))
for i in range(9):
    if i==0:
      axes[i//3,i%3].imshow(tf.expand_dims(first_image, 0)[0]/ 255)
      axes[i//3,i%3].set_title('Original Image')
    else:
      augmented_image = layers.RandomFlip('horizontal_and_vertical')(tf.expand_dims(first_image, 0))
      axes[i//3,i%3].imshow(augmented_image[0] / 255)
      axes[i//3,i%3].set_title(f'Flipped Image {i}')
    axes[i//3,i%3].axis('off')
```


    
![png](../images/image-classification/PIC16B_Blog_Post_4_27_0.png)
    


### Random Rotation

Now we show the result of rotations applied to the same image above. Again the first one shows the orignial image.


```python
fig, axes = plt.subplots(3,3, figsize=(10,10))
for i in range(9):
    if i==0:
      axes[i//3,i%3].imshow(tf.expand_dims(first_image, 0)[0]/ 255)
      axes[i//3,i%3].set_title('Original Image')
    else:
      augmented_image = layers.RandomRotation(0.5)(tf.expand_dims(first_image, 0))
      axes[i//3,i%3].imshow(augmented_image[0] / 255)
      axes[i//3,i%3].set_title(f'Rotated Image {i}')
    axes[i//3,i%3].axis('off')
```


    
![png](../images/image-classification/PIC16B_Blog_Post_4_30_0.png)
    


### Modeling with both random flip and rotation

We incorporate this random flips and rotations into our model2 and evaluate its performance.


```python
model2 = models.Sequential([
    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((4, 4)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(2) # number of classes
])
```


```python
model2.summary()
```

    Model: "sequential_9"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_28 (Conv2D)          (None, 158, 158, 64)      1792      
                                                                     
     random_flip_63 (RandomFlip)  (None, 158, 158, 64)     0         
                                                                     
     random_rotation_51 (RandomR  (None, 158, 158, 64)     0         
     otation)                                                        
                                                                     
     max_pooling2d_20 (MaxPoolin  (None, 79, 79, 64)       0         
     g2D)                                                            
                                                                     
     conv2d_29 (Conv2D)          (None, 77, 77, 64)        36928     
                                                                     
     max_pooling2d_21 (MaxPoolin  (None, 19, 19, 64)       0         
     g2D)                                                            
                                                                     
     conv2d_30 (Conv2D)          (None, 17, 17, 128)       73856     
                                                                     
     max_pooling2d_22 (MaxPoolin  (None, 8, 8, 128)        0         
     g2D)                                                            
                                                                     
     conv2d_31 (Conv2D)          (None, 6, 6, 128)         147584    
                                                                     
     flatten_8 (Flatten)         (None, 4608)              0         
                                                                     
     dense_16 (Dense)            (None, 128)               589952    
                                                                     
     dense_17 (Dense)            (None, 2)                 258       
                                                                     
    =================================================================
    Total params: 850,370
    Trainable params: 850,370
    Non-trainable params: 0
    _________________________________________________________________



```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

```


```python
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 14s 195ms/step - loss: 2.5289 - accuracy: 0.5260 - val_loss: 0.6741 - val_accuracy: 0.5532
    Epoch 2/20
    63/63 [==============================] - 12s 191ms/step - loss: 0.6856 - accuracy: 0.5515 - val_loss: 0.6643 - val_accuracy: 0.6139
    Epoch 3/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.6775 - accuracy: 0.5815 - val_loss: 0.6615 - val_accuracy: 0.5755
    Epoch 4/20
    63/63 [==============================] - 13s 194ms/step - loss: 0.6708 - accuracy: 0.5750 - val_loss: 0.6426 - val_accuracy: 0.6460
    Epoch 5/20
    63/63 [==============================] - 12s 191ms/step - loss: 0.6627 - accuracy: 0.5915 - val_loss: 0.6623 - val_accuracy: 0.5903
    Epoch 6/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.6431 - accuracy: 0.6305 - val_loss: 0.6013 - val_accuracy: 0.6696
    Epoch 7/20
    63/63 [==============================] - 12s 191ms/step - loss: 0.6448 - accuracy: 0.6230 - val_loss: 0.6606 - val_accuracy: 0.5978
    Epoch 8/20
    63/63 [==============================] - 14s 209ms/step - loss: 0.6583 - accuracy: 0.6135 - val_loss: 0.6603 - val_accuracy: 0.6176
    Epoch 9/20
    63/63 [==============================] - 12s 190ms/step - loss: 0.6636 - accuracy: 0.6215 - val_loss: 0.6155 - val_accuracy: 0.6869
    Epoch 10/20
    63/63 [==============================] - 12s 189ms/step - loss: 0.6292 - accuracy: 0.6565 - val_loss: 0.6155 - val_accuracy: 0.6535
    Epoch 11/20
    63/63 [==============================] - 12s 190ms/step - loss: 0.6277 - accuracy: 0.6510 - val_loss: 0.6260 - val_accuracy: 0.6485
    Epoch 12/20
    63/63 [==============================] - 12s 193ms/step - loss: 0.6049 - accuracy: 0.6815 - val_loss: 0.6137 - val_accuracy: 0.6696
    Epoch 13/20
    63/63 [==============================] - 14s 204ms/step - loss: 0.6090 - accuracy: 0.6755 - val_loss: 0.6034 - val_accuracy: 0.6671
    Epoch 14/20
    63/63 [==============================] - 13s 196ms/step - loss: 0.6029 - accuracy: 0.6680 - val_loss: 0.5637 - val_accuracy: 0.6968
    Epoch 15/20
    63/63 [==============================] - 12s 191ms/step - loss: 0.6096 - accuracy: 0.6720 - val_loss: 0.6007 - val_accuracy: 0.6733
    Epoch 16/20
    63/63 [==============================] - 13s 206ms/step - loss: 0.6192 - accuracy: 0.6660 - val_loss: 0.6320 - val_accuracy: 0.6448
    Epoch 17/20
    63/63 [==============================] - 13s 190ms/step - loss: 0.6105 - accuracy: 0.6580 - val_loss: 0.5976 - val_accuracy: 0.6646
    Epoch 18/20
    63/63 [==============================] - 12s 190ms/step - loss: 0.5906 - accuracy: 0.6830 - val_loss: 0.5981 - val_accuracy: 0.6658
    Epoch 19/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.5676 - accuracy: 0.7170 - val_loss: 0.5838 - val_accuracy: 0.7092
    Epoch 20/20
    63/63 [==============================] - 12s 190ms/step - loss: 0.5975 - accuracy: 0.6930 - val_loss: 0.5864 - val_accuracy: 0.6782



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fb4475450d0>




    
![png](../images/image-classification/PIC16B_Blog_Post_4_37_1.png)
    


The validation accuracy of model2 stabilized **between 65% and 71%** during training, which is higher than our model1 of 57%. Note, due to the higher complexity of input data here, we increased the number of layers and number of filters. In this model, we also get a very similar score between the training and validation score, which means that we are not overfitting our model.

## Data Preprocessing

Sometimes, transformations on the dataset can also improve performance. For example, by scaling the values to betweeen 0 and 1. The following code creates a `preprocessor` which helps handle such situations. We incorporate it into our model3.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model3 = models.Sequential([
    preprocessor,
    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((4, 4)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(2) # number of classes
])
```


```python
model3.summary()
```

    Model: "sequential_10"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     conv2d_32 (Conv2D)          (None, 158, 158, 64)      1792      
                                                                     
     random_flip_64 (RandomFlip)  (None, 158, 158, 64)     0         
                                                                     
     random_rotation_52 (RandomR  (None, 158, 158, 64)     0         
     otation)                                                        
                                                                     
     max_pooling2d_23 (MaxPoolin  (None, 79, 79, 64)       0         
     g2D)                                                            
                                                                     
     conv2d_33 (Conv2D)          (None, 77, 77, 64)        36928     
                                                                     
     max_pooling2d_24 (MaxPoolin  (None, 19, 19, 64)       0         
     g2D)                                                            
                                                                     
     conv2d_34 (Conv2D)          (None, 17, 17, 128)       73856     
                                                                     
     max_pooling2d_25 (MaxPoolin  (None, 8, 8, 128)        0         
     g2D)                                                            
                                                                     
     conv2d_35 (Conv2D)          (None, 6, 6, 128)         147584    
                                                                     
     flatten_9 (Flatten)         (None, 4608)              0         
                                                                     
     dense_18 (Dense)            (None, 128)               589952    
                                                                     
     dense_19 (Dense)            (None, 2)                 258       
                                                                     
    =================================================================
    Total params: 850,370
    Trainable params: 850,370
    Non-trainable params: 0
    _________________________________________________________________



```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

```


```python
history3 = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 14s 198ms/step - loss: 0.6988 - accuracy: 0.5055 - val_loss: 0.6927 - val_accuracy: 0.4926
    Epoch 2/20
    63/63 [==============================] - 13s 194ms/step - loss: 0.6917 - accuracy: 0.5320 - val_loss: 0.6929 - val_accuracy: 0.5248
    Epoch 3/20
    63/63 [==============================] - 12s 193ms/step - loss: 0.6824 - accuracy: 0.5635 - val_loss: 0.6591 - val_accuracy: 0.6151
    Epoch 4/20
    63/63 [==============================] - 12s 193ms/step - loss: 0.6476 - accuracy: 0.6205 - val_loss: 0.6278 - val_accuracy: 0.6213
    Epoch 5/20
    63/63 [==============================] - 12s 194ms/step - loss: 0.6108 - accuracy: 0.6595 - val_loss: 0.5845 - val_accuracy: 0.6869
    Epoch 6/20
    63/63 [==============================] - 13s 194ms/step - loss: 0.6002 - accuracy: 0.6795 - val_loss: 0.5846 - val_accuracy: 0.6757
    Epoch 7/20
    63/63 [==============================] - 13s 210ms/step - loss: 0.5795 - accuracy: 0.6900 - val_loss: 0.5422 - val_accuracy: 0.7277
    Epoch 8/20
    63/63 [==============================] - 14s 207ms/step - loss: 0.5473 - accuracy: 0.7165 - val_loss: 0.5881 - val_accuracy: 0.6683
    Epoch 9/20
    63/63 [==============================] - 14s 204ms/step - loss: 0.5536 - accuracy: 0.7210 - val_loss: 0.5452 - val_accuracy: 0.7191
    Epoch 10/20
    63/63 [==============================] - 13s 197ms/step - loss: 0.5338 - accuracy: 0.7340 - val_loss: 0.5840 - val_accuracy: 0.7191
    Epoch 11/20
    63/63 [==============================] - 13s 194ms/step - loss: 0.5337 - accuracy: 0.7250 - val_loss: 0.5525 - val_accuracy: 0.7203
    Epoch 12/20
    63/63 [==============================] - 13s 193ms/step - loss: 0.5112 - accuracy: 0.7630 - val_loss: 0.5547 - val_accuracy: 0.7475
    Epoch 13/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.4904 - accuracy: 0.7695 - val_loss: 0.5238 - val_accuracy: 0.7450
    Epoch 14/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.4802 - accuracy: 0.7705 - val_loss: 0.5240 - val_accuracy: 0.7488
    Epoch 15/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.4931 - accuracy: 0.7600 - val_loss: 0.5077 - val_accuracy: 0.7760
    Epoch 16/20
    63/63 [==============================] - 12s 192ms/step - loss: 0.4624 - accuracy: 0.7850 - val_loss: 0.5081 - val_accuracy: 0.7636
    Epoch 17/20
    63/63 [==============================] - 13s 195ms/step - loss: 0.4642 - accuracy: 0.7790 - val_loss: 0.4909 - val_accuracy: 0.7723
    Epoch 18/20
    63/63 [==============================] - 12s 193ms/step - loss: 0.4341 - accuracy: 0.8020 - val_loss: 0.4755 - val_accuracy: 0.7983
    Epoch 19/20
    63/63 [==============================] - 12s 193ms/step - loss: 0.4424 - accuracy: 0.7900 - val_loss: 0.5220 - val_accuracy: 0.7735
    Epoch 20/20
    63/63 [==============================] - 13s 196ms/step - loss: 0.4180 - accuracy: 0.8040 - val_loss: 0.5258 - val_accuracy: 0.7698



```python
plt.plot(history3.history["accuracy"], label = "training")
plt.plot(history3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fb44cbdbad0>




    
![png](../images/image-classification/PIC16B_Blog_Post_4_46_1.png)
    


After addnig the preprocessor layer, the validation accuracy of model3 stabilized **between 75% and 79%** during training, which is higher than both model1 and model2. In this model, again we also get a very similar score between the training and validation score, which means that we are not overfitting our model.

## Transfer Learning

Transfer leanring is a technique to build on top of pre-existing models. In this case, we first download `MobileNetV2` and include it as a layer in our model4. We also included the `preprocessor`, `RandomFlip` and `RandomRotation` from previous models in our model4. 


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step



```python
model4 = models.Sequential([
    preprocessor,
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    base_model_layer,
    layers.GlobalAveragePooling2D(),
    layers.Dense(2) # number of classes
])
```


```python
model4.summary()
```

    Model: "sequential_15"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_69 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_57 (RandomR  (None, 160, 160, 3)      0         
     otation)                                                        
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_average_pooling2d (G  (None, 1280)             0         
     lobalAveragePooling2D)                                          
                                                                     
     dense_27 (Dense)            (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________



```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

```


```python
history4 = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 11s 113ms/step - loss: 0.2498 - accuracy: 0.8965 - val_loss: 0.0584 - val_accuracy: 0.9851
    Epoch 2/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.1421 - accuracy: 0.9390 - val_loss: 0.0516 - val_accuracy: 0.9864
    Epoch 3/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.1388 - accuracy: 0.9435 - val_loss: 0.0458 - val_accuracy: 0.9876
    Epoch 4/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.1092 - accuracy: 0.9525 - val_loss: 0.0533 - val_accuracy: 0.9839
    Epoch 5/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0969 - accuracy: 0.9585 - val_loss: 0.0414 - val_accuracy: 0.9864
    Epoch 6/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0944 - accuracy: 0.9615 - val_loss: 0.0407 - val_accuracy: 0.9876
    Epoch 7/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.0955 - accuracy: 0.9600 - val_loss: 0.0359 - val_accuracy: 0.9851
    Epoch 8/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0895 - accuracy: 0.9630 - val_loss: 0.0330 - val_accuracy: 0.9889
    Epoch 9/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.1051 - accuracy: 0.9580 - val_loss: 0.0444 - val_accuracy: 0.9851
    Epoch 10/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.0948 - accuracy: 0.9595 - val_loss: 0.0354 - val_accuracy: 0.9876
    Epoch 11/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0804 - accuracy: 0.9660 - val_loss: 0.0356 - val_accuracy: 0.9876
    Epoch 12/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0784 - accuracy: 0.9705 - val_loss: 0.0451 - val_accuracy: 0.9851
    Epoch 13/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0847 - accuracy: 0.9645 - val_loss: 0.0402 - val_accuracy: 0.9876
    Epoch 14/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.0815 - accuracy: 0.9675 - val_loss: 0.0423 - val_accuracy: 0.9864
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0729 - accuracy: 0.9725 - val_loss: 0.0416 - val_accuracy: 0.9864
    Epoch 16/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.0781 - accuracy: 0.9725 - val_loss: 0.0496 - val_accuracy: 0.9814
    Epoch 17/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0715 - accuracy: 0.9710 - val_loss: 0.0386 - val_accuracy: 0.9851
    Epoch 18/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.0773 - accuracy: 0.9715 - val_loss: 0.0483 - val_accuracy: 0.9827
    Epoch 19/20
    63/63 [==============================] - 6s 99ms/step - loss: 0.0677 - accuracy: 0.9730 - val_loss: 0.0401 - val_accuracy: 0.9864
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.0705 - accuracy: 0.9695 - val_loss: 0.0556 - val_accuracy: 0.9777



```python
plt.plot(history4.history["accuracy"], label = "training")
plt.plot(history4.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fb4462bbc50>




    
![png](../images/image-classification/PIC16B_Blog_Post_4_55_1.png)
    


The validation accuracy of model4 stabilized **between 97% and 99%** during training, which is higher than all of the previous models. In this model, the validation accuracy is consistenly high while the traning accuracy starts lower at around 90% and gradually increases to around 97%. Again we don't see signs of overfitting.

## Score on Test data

At last, we pick our best model (model4 with transfer learning) and evaluate our model performance on the `test_dataset`.


```python
loss, accuracy = model4.evaluate(test_dataset)
```

    6/6 [==============================] - 2s 123ms/step - loss: 0.0365 - accuracy: 0.9844


The results show that we are able to achieve an accuracy of 98.44% on the test dataset, which has a high accuracy. Much than our baseline!


